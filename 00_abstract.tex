\begin{abstract}
Foundation models like BiomedParse and SAM have improved biomedical segmentation with text-to-mask capabilities, but they often fail when prompts shift from simple anatomical targets (e.g., ``kidney'') to fine-grained pathological descriptions (e.g., ``necrotic tumor core''). We refer to this degradation as \textit{semantic collapse}. We identify that this stems from CLIP's inability to represent negation geometrically---prompts like ``excluding tumor'' still activate tumor-related features. In this paper, we introduce \textbf{SemantiBench}, a dataset of 100,000+ prompt-mask pairs, and propose \textbf{FreqMedCLIP}, a segmentation model that explicitly separates target and avoidance signals. Unlike standard methods, our model decouples prompts into Target and Avoidance streams and enforces constraints via a \textbf{logical gate}. We also introduce an \textbf{exclusion loss} that supervises the gate to act as a precise detector for forbidden regions. Our model achieves a Dice score of 0.86 on complex exclusionary queries ($L_3$), significantly outperforming the baseline (Dice 0.60). Current models fail this test; they ignore ``excluding'' and segment everything.

\keywords{Medical Image Segmentation \and Foundation Models \and Semantic Robustness \and Benchmarking}
\end{abstract}
