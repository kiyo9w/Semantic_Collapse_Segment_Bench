\begin{abstract}
Foundation models like BiomedParse and SAM have improved biomedical segmentation with text-to-mask capabilities, but they often fail when prompts shift from simple anatomical targets (e.g., "kidney") to fine-grained pathological descriptions (e.g., "necrotic tumor core"). We refer to this degradation as semantic collapse. Current benchmarks rely on static class labels and do not measure this specific failure mode. In this paper, we introduce SemantiBench, a dataset of 100,000+ prompt-mask pairs generated through an automated pipeline to test linguistic robustness. Using this benchmark, we show that current models lose 29\% performance on complex clinical queries ($L_3$). We propose FreqMedClip, an architecture that uses cross-modal semantic gating to separate spatial localization from attribute verification. Our experiments show that FreqMedClip maintains performance (Prompt Sensitivity Score $< 0.05$) in cases where baseline models fail.

\keywords{Medical Image Segmentation \and Foundation Models \and Semantic Robustness \and Benchmarking}
\end{abstract}
