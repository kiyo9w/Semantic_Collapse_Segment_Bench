\begin{abstract}
Foundation models like BiomedParse and SAM have improved biomedical segmentation with text-to-mask capabilities, but they often fail when prompts shift from simple anatomical targets (e.g., "kidney") to fine-grained pathological descriptions (e.g., "necrotic tumor core"). We refer to this degradation as \textit{semantic collapse}. Current benchmarks rely on static class labels and do not measure this specific failure mode. In this paper, we introduce SemantiBench, a dataset of 100,000+ prompt-mask pairs. We show that SOTA models like BiomedParse lose 29\% performance on complex exclusionary queries ($L_3$). We propose \textbf{FreqMedCLIP}, a hybrid architecture that integrates \textbf{Haar Wavelet spectral analysis} with a foundation model backbone. By explicitly encoding high-frequency boundary signals, FreqMedCLIP maintains high precision. \textbf{Crucially, our model achieves a Dice score of 0.81 on complex queries, significantly outperforming the baseline (Dice 0.60), while maintaining a prompt sensitivity score (PSS) of $< 0.05$.}

\keywords{Medical Image Segmentation \and Foundation Models \and Semantic Robustness \and Benchmarking}
\end{abstract}
