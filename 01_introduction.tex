\section{Introduction}
\label{sec:intro}

Foundation models such as BiomedParse~\cite{zhao2024biomedparse} and MedSAM~\cite{ma2024medsam} can now segment biomedical objects using natural language. However, strict adherence to clinical prompts remains a problem. While these models succeed at atomic queries ($L_1$) like ``kidney'', they often fail to comply with complex constraints ($L_3$) such as ``kidney excluding the renal pelvis''.

This failure often stems from ``semantic collapse'', where the model ignores key logical or descriptive modifiers and incorrectly defaults to the generic object definition. For example, when prompted with ``necrotic tumor core'', many state-of-the-art models ignore the adjective ``necrotic'' and segment the entire tumor. This behavior is dangerous in clinical settings where precise sub-region targeting is required.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/L1L2L3.pdf}
    \caption{\textbf{Semantic failure modes.} (A) Existing foundation models (BiomedParse) fail to process the adjective ``necrotic,'' incorrectly segmenting the entire tumor mass~\cite{zhao2024biomedparse}. (B) SemantiBench evaluates robustness across three linguistic levels ($L_1-L_3$). (C) FreqMedClip uses cross-modal gating to isolate the necrotic core.}
    \label{fig:teaser}
\end{figure}

This paper introduces three contributions to address this problem. First, we propose \textbf{SemantiBench}, a protocol to measure two distinct forms of robustness: (1) \textbf{Descriptive Invariance} ($L_2$), which checks if models output consistent masks for synonymous prompts (e.g., ``kidney'' vs. ``bean-shaped organ''); and (2) \textbf{Logical Compliance} ($L_3$), which tests if models correctly modify the segmentation mask when prompts impose exclusionary constraints.

To quantify these failures, we introduce the \textbf{Prompt Sensitivity Score (PSS)}. We find that current models exhibit a PSS gap of up to 0.29, indicating poor compliance with complex instructions.

Finally, we present \textbf{FreqMedCLIP}, an architecture designed to solve this ambiguity. We argue that CLIP embeddings suffer from what we call ``affirmative bias''---the encoder treats all words in a prompt as inclusion signals, even negation operators like ``excluding.'' FreqMedCLIP addresses this by decoupling the text into Target and Avoidance streams. We introduce a \textbf{logical gating} mechanism that forces the exclusion through multiplicative gating, suppressing the forbidden region at the feature level. This approach, combined with explicit exclusion loss, reduces the PSS to 0.12, significantly improving prompt compliance.
