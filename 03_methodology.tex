\section{Methodology}
\label{sec:method}

\begin{figure}[t]
    \centering
    \makebox[\textwidth][c]{\includegraphics[width=1.1\textwidth]{"figures/Freqmedclip Architecture.pdf"}} % Keep original figure
    \caption{\textbf{FreqMedClip Architecture.} The model features two streams: (1) A visual backbone (BiomedCLIP) to extract global semantic context, and (2) A High-Frequency Encoder (Wavelet-based) to capture texture and boundary details associated with specific attributes (e.g., ``necrotic,'' ``spiculated''). The Cross-Modal Semantic Gating (CMSG) mechanism uses the text prompt to actively filter the high-frequency stream, isolating only the relevant sub-structures.}
    \label{fig:architecture}
\end{figure}

We designed FreqMedCLIP to solve the ``Frequency-Semantic Gap.'' We hypothesize that semantic attributes (like ``necrotic'' or ``nodular'') are often encoded in high-frequency signals that standard Vision Transformers smooth out. Therefore, our model treats the high-frequency stream not just as a boundary detector, but as the raw material for semantic reasoning.

\paragraph{3.1. High-Frequency Attribute Encoder.}
Standard ViTs suffer from a ``low-pass'' bias, making them blind to texture-defined semantics~\cite{park2022vision}. Recent work has shown that explicit frequency modeling significantly boosts segmentation of subtle structures~\cite{duwsnet2025,huang2025frequency}. We address this by explicitly encoding the high-frequency spectrum. We use a \textbf{Discrete Wavelet Transform (DWT)} to decompose the input $\mathbf{X}$ into four spectral sub-bands:
\begin{equation}
    \mathbf{X}_{freq} = \text{DWT}(\mathbf{X}) = \{ \mathbf{X}_{LL}, \mathbf{X}_{LH}, \mathbf{X}_{HL}, \mathbf{X}_{HH} \}
\end{equation}
Here, $\{ \mathbf{X}_{LH}, \mathbf{X}_{HL}, \mathbf{X}_{HH} \}$ capture the vertical, horizontal, and diagonal high-frequency detailsâ€”precisely where features like tissue texture (necrotic vs. vital) and margin type (spiculated vs. smooth) reside. These 12 spectral channels feed a \textbf{ConvNeXt-Tiny} encoder, which provides the ``semantic raw material'' that the text gate will later act upon.

\paragraph{3.2. Dual-Stream Encoder.}
We use the \textbf{BiomedCLIP ViT-B/16} to extract the global semantic context. To handle compositional queries (e.g., ``Kidney excluding tumor''), we use a semantic decomposition strategy. The input prompt is split into $P_{pos}$ (``Kidney'') and $P_{neg}$ (``Tumor''). These are encoded into two distinct embedding vectors, $\mathbf{E}_{pos}$ and $\mathbf{E}_{neg}$, ensuring that the avoidance signal remains independent from the target representation.

\paragraph{3.3. Logical Gating.}
We propose a \textbf{Logical Gate}. Unlike standard cross-attention which mixes signals, this gate operates on strict exclusion logic. The high-frequency feature map $\mathbf{F}_{freq}$ is processed by two parallel paths:

\begin{itemize}
    \item \textbf{Inclusion Path ($\mathbf{G}_{inc}$):} Activates regions matching $\mathbf{E}_{pos}$.
    \item \textbf{Exclusion Path ($\mathbf{G}_{exc}$):} Activates regions matching $\mathbf{E}_{neg}$.
\end{itemize}

The final feature refinement enforces the logical constraint: include regions matching $P_{pos}$ while suppressing regions matching $P_{neg}$:
\begin{equation}
    \mathbf{F}_{refined} = \mathbf{F}_{freq} \odot \mathbf{G}_{inc} \odot (1 - \mathbf{G}_{exc})
\end{equation}
This operator suppresses features that align with the forbidden concept.

\paragraph{3.4. Exclusion Loss.}
Implicit supervision fails to teach the model negation. Therefore, we introduce an \textbf{exclusion loss} ($L_{excl}$). During training, we utilize multi-class ground truth to directly supervise the Exclusion Path ($\mathbf{G}_{exc}$), forcing it to detect the excluded class with high accuracy. This ensures the suppression term $(1 - \mathbf{G}_{exc})$ receives strong gradient signals during training.
\begin{equation}
    L_{total} = L_{Dice}(\mathbf{Y}_{pred}, \mathbf{Y}_{target}) + \lambda L_{BCE}(\mathbf{G}_{exc}, \mathbf{Y}_{avoid})
\end{equation}
