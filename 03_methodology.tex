\section{Methodology}
\label{sec:method}

\begin{figure}[t]
    \centering
    % ==========================================================================================
    % [VISUALIZATION SPECIFICATION: The System Architecture Figure (Mandatory)]
    % Why: Reviewers look at this diagram to understand "Dual-Stream" and "Gating" mechanism.
    % ------------------------------------------------------------------------------------------
    % VISUAL DESIGN:
    % - Left Side (Inputs): Show the Input Image splitting into two paths.
    % - Top Path (Stream 1 - Semantic):
    %   * Show a block labeled "BiomedCLIP (ViT-B/16)".
    %   * Color it Gray (Frozen) but highlight last few layers in Orange (Partial Finetuning).
    %   * Output arrow labeled F_sem.
    % - Bottom Path (Stream 2 - Frequency):
    %   * Show a small block labeled "DWT" splitting the image into 4 sub-squares.
    %   * Feed these into a block labeled "ConvNeXt-Tiny".
    %   * Output arrow labeled F_freq.
    % - Center (The Innovation):
    %   * Show the Cross-Modal Semantic Gating (CMSG) module.
    %   * Depict the Text Prompt entering here and interacting with F_sem to generate the gate alpha.
    %   * Show alpha multiplying with F_freq (the "Gating" action).
    % - Right Side (Output): The fused features flowing into a segmentation head to produce final mask.
    % ------------------------------------------------------------------------------------------
    
    \fbox{\begin{minipage}[c][0.4\textwidth][c]{0.95\textwidth}
        \centering
        \ttfamily
        \Large [PLACEHOLDER: SYSTEM ARCHITECTURE FIGURE]\\
        \normalsize
        See comments for detailed visualization specifications
    \end{minipage}}

    \caption{\textbf{FreqMedClip Architecture.} The model features two streams: (1) A visual backbone (BiomedCLIP) with a feature pyramid to extract multi-scale spatial features, and (2) A linguistic disentanglement module that parses prompts into Nominal and Attribute embeddings. The Cross-Modal Semantic Gating (CMSG) mechanism dynamically filters these visual features based on attribute constraints.}
    \label{fig:architecture}
\end{figure}

We attribute the phenomenon of \textit{Semantic Collapse} to the "Holistic Embedding Bias" in standard Vision-Language models. Architectures like BiomedParse utilize CLIP-style encoders that pool the entire prompt into a single vector ($\mathbf{e}_{text}$). In this compressed latent space, the semantic mass of strong anatomical nouns (e.g., "Tumor") often overshadows subtle adjectival constraints (e.g., "Necrotic"), causing the model to ignore the latter.

To resolve this, we introduce \textbf{FreqMedClip}, an architecture designed to explicitly decouple \textit{Spatial Localization} (Where is the object?) from \textit{Attribute Verification} (What are its properties?).

\paragraph{3.1. Overview: The Dual-Path Gating Architecture.}
As illustrated in Fig.~\ref{fig:architecture}, FreqMedClip departs from the standard "Early Fusion" paradigm. Instead of fusing image and text immediately, we maintain two distinct processing streams that intersect only via a novel \textbf{Cross-Modal Semantic Gating (CMSG)} mechanism.

\paragraph{Stream 1: The Visual Backbone.}
We employ a \textbf{BiomedCLIP ViT-B/16} encoder to extract multi-scale visual features. Unlike standard fine-tuning, we utilize a \textit{Feature Pyramid Strategy}, extracting features from Transformer Blocks $\{3, 6, 9, 12\}$. Let $\mathbf{F}_{v}^{(i)} \in \mathbb{R}^{H_i \times W_i \times C}$ denote the visual feature map at scale $i$. These maps capture potential regions of interest (ROI) based on shape and boundary cues.

\paragraph{Stream 2: The Linguistic Disentanglement Module.}
This module is the core innovation. Rather than encoding the prompt $T$ as a monolithic vector, we parse $T$ into two distinct components:
\begin{itemize}
    \item \textbf{The Nominal Embedding ($\mathbf{e}_{noun}$):} Derived from the anatomical target (e.g., "Tumor", "Lesion"). This guides global localization.
    \item \textbf{The Attribute Embedding ($\mathbf{e}_{attr}$):} Derived from pathological descriptors (e.g., "Necrotic", "Hypodense", "Spiculated"). This acts as a semantic filter.
\end{itemize}
We utilize a lightweight BERT-based dependency parser to extract these tokens, which are then projected into the visual latent space via a learned MLP: $\mathbf{z}_{attr} = \text{MLP}(\mathbf{e}_{attr})$.

\paragraph{3.2. Cross-Modal Semantic Gating (CMSG).}
The CMSG module enforces semantic robustness by using the attribute embedding to "gate" the visual features. If a visual region does not match the texture/intensity profile implied by $\mathbf{z}_{attr}$ (e.g., a bright region when the prompt implies "dark/necrotic"), the gate suppresses it.
Mathematically, for a visual feature map $\mathbf{F}_{v}$, we compute a spatial attention map $\mathbf{A}_{gate}$:
\begin{equation}
    \mathbf{A}_{gate} = \sigma \left( \text{Conv}_{1\times1} \left( \mathbf{F}_{v} \odot \mathbf{z}_{attr} \right) \right)
\end{equation}
where $\odot$ denotes channel-wise multiplication and $\sigma$ is the sigmoid activation. The refined feature map is:
\begin{equation}
    \mathbf{F}_{refined} = \mathbf{F}_{v} \cdot (1 + \mathbf{A}_{gate})
\end{equation}
This mechanism effectively "turns off" activations in the tumor region that do not correspond to the "necrotic" description, solving the over-segmentation problem.

\paragraph{3.3. Attribute Consistency Loss.}
To ensure the model actually learns to use the CMSG module (rather than bypassing it), we introduce an auxiliary \textbf{Attribute Consistency Loss ($\mathcal{L}_{attr}$)}. We maximize the cosine similarity between the pooled visual features of the predicted mask ($\mathbf{v}_{mask}$) and the attribute embedding ($\mathbf{z}_{attr}$):
\begin{equation}
    \mathcal{L}_{total} = \mathcal{L}_{Dice} + \mathcal{L}_{CE} + \lambda \cdot (1 - \text{sim}(\mathbf{v}_{mask}, \mathbf{z}_{attr}))
\end{equation}
This explicitly penalizes the model if the segmented region does not visually match the textual description.
