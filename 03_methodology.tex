\section{Methodology}
\label{sec:method}

We find that standard Vision Transformers (ViT) often act as low-pass filters~\cite{park2022vision}, which limits their ability to capture the high-frequency spatial details needed for fine-grained adjectives like "spiculated" or "necrotic." SemantiSeg addresses this with a dual-stream architecture that combines semantic and frequency features.

\paragraph{Stream 1: Semantic Context Encoder.}
We use a BiomedCLIP ViT-B/16 backbone to extract global semantic features. We apply a partial finetuning strategy, unfreezing transformer layers 9-11 so the model can adapt to medical ontologies without losing generalized knowledge. Given an input image $X \in \mathbb{R}^{H \times W \times 3}$, this stream produces a feature map $F_{sem} \in \mathbb{R}^{14 \times 14 \times 512}$.

\paragraph{Stream 2: Frequency-Aware Spatial Encoder.}
To recover spatial information, we use a dedicated frequency stream. First, the input $X$ undergoes a discrete wavelet transform (DWT) using 2D Haar wavelets, decomposing the image into four spectral components:
\begin{equation}
    \text{DWT}(X) = \{ X_{LL}, X_{LH}, X_{HL}, X_{HH} \}
\end{equation}
Here, $X_{LL}$ is the low-frequency approximation, while $\{X_{LH}, X_{HL}, X_{HH}\}$ represent the high-frequency details. These components are concatenated to form a 12-channel tensor $X_{freq}$, which is then processed by a ConvNeXt-Tiny encoder. Feeding the frequency bands directly allows the model to learn separate representations for shape ($LL$) and texture ($HH$).

\paragraph{Cross-Modal Semantic Gating.}
We fuse these streams using a gating mechanism modulated by the text embedding. Naive concatenation can lead to one stream dominating the other. Instead, we use the text embedding $E_{text}$ to control the frequency features. Let $F_{freq}^{(i)}$ be the feature map from the $i$-th stage of the ConvNeXt encoder. We compute a gating scalar $\alpha \in [0, 1]$:
\begin{equation}
    \alpha = \sigma ( \text{MLP}(E_{text} \odot \text{GlobalAvgPool}(F_{sem})) )
\end{equation}
The fused feature map $F_{fused}$ is then:
\begin{equation}
    F_{fused} = F_{sem}^{up} + (1 + \alpha) \cdot F_{freq}
\end{equation}
This allows the network to emphasize the frequency stream when the prompt requires it, such as for texture-heavy descriptions.

\paragraph{Optimization.}
The training objective combines Dice loss and a hard negative component: $\mathcal{L}_{total} = \lambda_1 \mathcal{L}_{Dice} + \lambda_2 \mathcal{L}_{DHN}$. We use a Dynamic Hard Negative (DHN) loss to address class imbalance. This loss increases the weight of pixels where the model confidence is low ($p < 0.5$):
\begin{equation}
    \mathcal{L}_{DHN} = - \sum_{i \in \Omega} w_i \cdot y_i \log(p_i), \quad \text{where } w_i = (1 - p_i)^\gamma
\end{equation}
This focuses formulation encourages the model to refine difficult boundary regions common in complex prompts.
