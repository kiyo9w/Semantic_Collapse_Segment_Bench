\section{Methodology}
\label{sec:method}

\begin{figure}[t]
    \centering
    % ==========================================================================================
    % [VISUALIZATION SPECIFICATION: The System Architecture Figure (Mandatory)]
    % Why: Reviewers look at this diagram to understand "Dual-Stream" and "Gating" mechanism.
    % ------------------------------------------------------------------------------------------
    % VISUAL DESIGN:
    % - Left Side (Inputs): Show the Input Image splitting into two paths.
    % - Top Path (Stream 1 - Semantic):
    %   * Show a block labeled "BiomedCLIP (ViT-B/16)".
    %   * Color it Gray (Frozen) but highlight last few layers in Orange (Partial Finetuning).
    %   * Output arrow labeled F_sem.
    % - Bottom Path (Stream 2 - Frequency):
    %   * Show a small block labeled "DWT" splitting the image into 4 sub-squares.
    %   * Feed these into a block labeled "ConvNeXt-Tiny".
    %   * Output arrow labeled F_freq.
    % - Center (The Innovation):
    %   * Show the Cross-Modal Semantic Gating (CMSG) module.
    %   * Depict the Text Prompt entering here and interacting with F_sem to generate the gate alpha.
    %   * Show alpha multiplying with F_freq (the "Gating" action).
    % - Right Side (Output): The fused features flowing into a segmentation head to produce final mask.
    % ------------------------------------------------------------------------------------------
    
    \fbox{\begin{minipage}[c][0.4\textwidth][c]{0.95\textwidth}
        \centering
        \ttfamily
        \Large [PLACEHOLDER: SYSTEM ARCHITECTURE FIGURE]\\
        \normalsize
        See comments for detailed visualization specifications
    \end{minipage}}

    \caption{\textbf{FreqMedClip Architecture.} The model features two streams: (1) A visual backbone (BiomedCLIP) with a feature pyramid to extract multi-scale spatial features, and (2) A linguistic disentanglement module that parses prompts into Nominal and Attribute embeddings. The Cross-Modal Semantic Gating (CMSG) mechanism dynamically filters these visual features based on attribute constraints.}
    \label{fig:architecture}
\end{figure}

We designed FreqMedCLIP as a dual-stream hybrid architecture to fix the generic "semantic collapse" seen in standard Vision Transformers. The model separates two distinct learning tasks: global semantic alignment (what is the object?) and local boundary delineation (where are the edges?).

As shown in Fig.~\ref{fig:architecture}, the system uses three coupled parts: a semantic stream (BiomedCLIP), a frequency stream (Wavelet-based), and a hyper-fusion decoder that projects semantic constraints into the frequency domain.

\paragraph{3.1 Frequency-Domain Visual Encoder}

Unlike standard vision transformers that operate solely in the RGB spatial domain, FreqMedCLIP explicitly decouples high-frequency structural details (boundaries) from low-frequency semantic information (shapes). We implement this via a hard-coded spectral decomposition layer.

\textbf{Discrete Wavelet Transform (DWT):}
Given an input image $\mathbf{X}$, we apply a 2D Haar Wavelet Transform to decompose the signal into four spectral sub-bands:
\begin{equation}
    \mathbf{X}_{freq} = \text{DWT}(\mathbf{X}) = \{ \mathbf{X}_{LL}, \mathbf{X}_{LH}, \mathbf{X}_{HL}, \mathbf{X}_{HH} \}
\end{equation}
where $\mathbf{X}_{LL}$ represents the low-frequency approximation, and $\{\mathbf{X}_{LH}, \mathbf{X}_{HL}, \mathbf{X}_{HH}\}$ capture vertical, horizontal, and diagonal high-frequency edge coefficients, respectively.

\textbf{12-Channel Spectral Input:}
Instead of downsampling, we concatenate these sub-bands channel-wise to form a spectral volume $\mathbf{V}_{spec} \in \mathbb{R}^{\frac{H}{2} \times \frac{W}{2} \times 12}$ (4 bands $\times$ 3 RGB channels). This tensor is fed into a modified \textbf{ConvNeXt-Tiny} encoder where the input stem has been surgically altered to accept 12 channels ($C_{in}=12$). This forces the network to learn filters that explicitly weight spectral texture information ($HH$ band) against semantic shape information ($LL$ band) from the first layer.

\paragraph{3.2. The Semantic Stream.}
We use the \textbf{BiomedCLIP ViT-B/16} encoder to extract semantic priors from the standard RGB image. To stop the model from forgetting medical concepts, we freeze most transformer blocks. We only unfreeze layers 3, 6, 9, and 11. This partial fine-tuning strategy produces a deep semantic embedding map $\mathbf{E}_{sem}$.

\paragraph{3.3. Cross-Modal Semantic Gating (CMSG).}
This module bridges the two streams. It handles the resolution difference between the ViT and the high-res frequency features.
First, we project the semantic feature map $\mathbf{E}_{sem}$ via a $1 \times 1$ convolution to match the channel dimension of the frequency features, then upsample it bi-linearly.
Instead of just concatenating them, we use a multiplicative "Hyper-Attention" gating mechanism. The semantic features act as a filter that can suppress frequency activations in regions that don't match the text prompt (like suppressing "rib cage edges" when we want "lung tumor").
\begin{equation}
    \mathbf{F}_{refined} = \mathbf{F}_{freq} \cdot \sigma(\mathbf{W}_g \cdot (\mathbf{F}_{freq} \odot \mathbf{E}_{sem}))
\end{equation}
This multiplicative interaction allows the gate to approach zero, effectively "turning off" edges that belong to the wrong object, solving the over-segmentation problem.

\paragraph{3.4. Differentiable Attribute Loss.}
To avoid the "Bag-of-Words" fallacy where spatial structure is lost, we replace standard Global Average Pooling with **Spatial-Weighted Attribute Pooling (SWAP)**. We use the attention map $\mathbf{A}_{gate}$ from the CMSG module to weight the pooling, ensuring the loss focuses only on the relevant anatomical regions.
\begin{equation}
    \mathbf{v}_{pooled} = \frac{\sum_{i,j} \mathbf{F}_{refined}^{(i,j)} \cdot \mathbf{A}_{gate}^{(i,j)}}{\sum_{i,j} \mathbf{A}_{gate}^{(i,j)}}
\end{equation}
We force this vector to align with the text embedding $\mathbf{z}_{attr}$ using a Contrastive Loss. Because $\mathbf{v}_{pooled}$ preserves the spatial focus of the gating mechanism, gradients flow smoothly back to update both encoders without relying on discrete masks.
